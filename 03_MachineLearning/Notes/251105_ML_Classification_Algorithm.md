# 머신러닝 기초 (지도학습 - 분류 알고리즘)

> 🗓️ **2025-11-05**  
> ✍🏼 **작성자 : unz**

## 📝 목차

1. Decision Tree
2. Naive Bayes
3. LDA
4. KNN
5. 알고리즘 비교

---

## 1. Decision Tree

> 데이터에 있는 규칙을 학습을 통해 트리 기반의 분류 규칙을 만드는 알고리즘

### 1-1. 트리의 구성요소

- 뿌리 노드(Root Node) : 전체 데이터셋이 시작되는 최상위 노드
- 중간 노드(Internal Node) : 데이터를 분리하는 기준(Feature)이 위치하는 중간 단계 노드
- 단말 노드(Leaf Node) : 더 이상 자식이 없고, 최종적인 결정(Class)이 내려지는 노드

### 1-2. Decision Tree 작동 원리

> 데이터의 특징(Feature)을 기준으로 데이터를 분할했을 때, 분할된 집단의 순수도(Purity)가 최대가 되도록 영역을 나누어 나간다.

### 1-3. Decision Tree 중단 조건

1. **완전 순수** : Gini = 0 또는 Entropy = 0, 모든 샘플이 같은 클래스
2. **최대 깊이 도달** : max_depth에 도달, 더이상 깊어질 수 없음
3. **샘플 수 부족** : 샘플 < min_samples_split, 나누기에는 데이터가 너무 적음
4. **개선 없음** : Information Gain ≈ 0, 나눠도 별로 좋아지지 않음

### 1-4. Purity & Gini Impurity & Entropy

- **Purity** : 한 노드에 포함된 데이터가 얼마나 하나의 클래스에 집중되어 있는지
  - 한 클래스만 존재 → Purity 높음 (완전 순수)
  - 여러 클래스가 섞여 있음 → Purity 낮음 (불순)
- **Gini Impurity** : 노드 안에 서로 다른 클래스가 얼마나 섞여 있는지 측정

  - 이 노드에서 아무 데이터나 뽑았을 때, 틀릴 가능성은?
  - 빨간공 100% : Gini = 0 (절대 틀리지 않음, 완전 순수)
  - 빨간공 50% / 파란공 50% : Gini = 0.5 (반은 틀릴 수 있음, 매우 불순)
  - 빨간공 90% / 파란공 10% : Gini ≈ 0.18 (대부분 맞음, 상당히 순수)

- **Entropy** : 노드의 무질서도를 측정
  - 이 노드에서 클래스 예측이 얼마나 헷갈리는가?
  - 빨간공 100% : Entropy = 0 (절대 헷갈리지 않음)
  - 빨간공 50% / 파란공 50% : Entropy = 1 (가장 헷갈림)
  - 빨간공 90% / 파란공 10% : EEntropy ≈ 0.47 (약간 헷갈림)

| 비교 |        Gini Impurity         |              Entropy               |
| :--: | :--------------------------: | :--------------------------------: |
| 공식 | $1 - \sum_{i=1}^{n} (p_i)^2$ | $- \sum_{i=1}^{n} p_i \log_2(p_i)$ |
| 장점 |         계산이 빠름          |         이론적으로 더 정확         |
| 범위 |           0 ~ 0.5            |              0 ~ 1.0               |
| 사용 |        sklearn 기본값        |           C4.5 알고리즘            |
| 특징 |          확률 기반           |           정보 이론 기반           |

### 1-5. Information Gain

> 어떤 기준으로 데이터를 나눴을 때 불순도가 얼마나 감소했는지 측정하는 지표  
> Information Gain = 상위 노드 불순도 - 하위 노드 불순도

```
직관적 이해
Before: 섞인 사탕 100개 (빨강 60, 파랑 40)
→ 불순도 = 0.48

After Split: "맛이 단가?"로 나눔
• Yes(40개): 빨강 10, 파랑 30 → 불순도 = 0.375
• No(60개): 빨강 50, 파랑 10 → 불순도 = 0.278
→ 가중 평균 불순도 = 0.4 × 0.375 + 0.6 × 0.278 = 0.317

Information Gain = 0.48 - 0.317 = 0.163
→ 불순도가 0.163 줄어듦 = 좋은 질문! ✓
```

### 1-6. Greedy Algorithm

> 탐욕적 알고리즘 : 현재 시점에서 가장 최적인 분기 조건(Information Gain이 가장 큰 것)을 선택하는 방식

### 1-7. Overfitting 해결방법

> 하이퍼파라미터 튜닝

- max_depth: 트리의 최대 깊이를 제한
- min_samples_split: 노드를 분할하기 위한 최소 샘플 데이터 수 제한 (너무 작은 노드 분할 방지)
- min_samples_leaf: 말단 노드가 되기 위한 최소 샘플 수 제한 (너무 특수한 규칙 방지)
- Pruning (가지치기)
  - Pre-pruning (사전) - 트리 생성 과정에서 특정 조건을 만족하면 노드를 더이상 분할하지 않음
  - Post-pruning (사후) - 트리를 다 만든 후, 영향력이 적은 가지를 제거하여 일반화 성능을 높임

## 2. Naive Bayes

> 베이즈 정리에 기반한 통계적 분류기법

- 모든 특성들이 서로 독립적이라고 가정(Naive)하기 때문에 계산이 매우 간단하고 빠르다.

```
[스팸 메일 분류]
메일에 "무료"라는 단어가 포함되어 있을 때, 이 메일이 스팸일지 정상일지 계산한다.
각 단어의 출현 빈도를  통해 확률을 예측한다.

P(스팸|메일) = P(메일|스팸) × P(스팸) / P(메일)
P(정상|메일) = P(메일|정상) × P(정상) / P(메일)
→ 두 확률을 비교해서 큰 쪽 선택!
```

### 2-1. Naive Bayes 종류

- Gaussian Naive Bayes : 연속형 데이터가 정규 분포를 따른다고 가정할 때 사용
- Multinomial Naive Bayes : 텍스트 분류와 같이 빈도수 데이터에 사용
- Bernoulli Naive Bayes : 이진 데이터에 사용 (단어 존재 여부)

## 3. LDA

> Linear Discriminant Analysis  
> 선형 판별 분석법

- 서로 다른 클래스의 데이터를 공간상에서 가장 잘 분리되도록 저차원 공간으로 축소하는 것
- 클래스 간 분산 최대화, 클래스 내 분산 최소화 조건을 동시에 만족하는 축을 찾는다.
  - **클래스 간 분산 최대화** : 클래스들의 중심(Mean)이 서로 최대한 멀리 떨어지게 한다.
  - **클래스 내 분산 최소화** : 같은 클래스에 속한 데이터들끼리는 최대한 뭉치게 한다.
- 차원 축소와 분류를 동시에 수행한다.
- 최대 (클래스 개수 - 1) 차원으로 축소한다.

## 4. KNN

> K-Nearest Neighbors
> 새로운 데이터가 들어오면 가장 가까운 K개의 이웃을 살펴보고 다수결로 클래스를 결정하는 알고리즘

### 4-1. K 값의 영향 및 최적 K 찾기

- K가 너무 작으면 (K=1)
  - 가장 가까운 이웃 1개만 확인
  - Overfitting (노이즈에 민감)
- K가 너무 크면 (K=50)
  - 많은 이웃을 고려
  - Underfitting (경계가 뭉개짐)
- 최적 K 찾기
  - 교차 검증(Cross-validation)을 통해 에러가 가장 적은 K를 선택
  - 동점을 방지하기 위해 보통 홀수를 선택

### 4-2. 거리 측정 방법

- Euclidean Distance: 직선 거리
- Manhattan Distance: 격자 거리
- Minkowski Distance: 유클리드와 맨해튼을 일반화한 거리 공식

## 5. 알고리즘 비교

| 알고리즘            | 장점                                      | 단점                                   | 사용 시기                                | 주의점                      |
| ------------------- | ----------------------------------------- | -------------------------------------- | ---------------------------------------- | --------------------------- |
| Decision Tree       | 해석이 쉬움<br> 데이터 스케일링 필요 없음 | Overfitting 위험 높음                  | 규칙 기반 결정                           | 하이퍼파라미터 튜닝         |
| Naive Bayes         | 계산 속도 빠름<br> 텍스트 데이터에 강함   | 변수 간 독립성 가정의 한계             | 텍스트 분류, 감성 분석                   | 독립성 가정이 맞는지 확인   |
| LDA                 | 분류 성능을 고려한<br> 차원 축소 가능     | 정규성 가정이 필요                     | 특징이 너무 많고 클래스 구분이 뚜렷할 때 | 데이터 분포 확인            |
| KNN                 | 모델 학습 필요 없음                       | 예측 시 연산 비용 높음<br> 차원의 저주 | 소규모 데이터, 복잡한 경계               | 반드시 데이터 스케일링 필요 |
| Logistic Regression | 선형 관계 파악 용이<br> 확률값 제공       | 선형적이지 않은 데이터에 취약          | 확률 필요                                | 다중공선성 확인             |
