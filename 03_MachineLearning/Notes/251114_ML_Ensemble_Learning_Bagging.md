# 머신러닝 심화 (앙상블 학습 - Bagging)

> 🗓️ **2025-11-14**  
> ✍🏼 **작성자 : unz**

## 📝 목차

1. Decision Tree의 강점과 근본적 한계
2. 앙상블 학습
3. Bagging
4. Out-of-Bag
5. Random Forest

---

## 1. Decision Tree의 강점과 근본적 한계

- 강점
  - 직관적 해석: if-then 규칙으로 의사결정 과정을 명확히 이해
  - 비선형 패턴: 복잡한 비선형 관계를 포착 가능
  - 전처리 불필요: 스케일링이나 정규화가 필요 없음
  - 범주형 변수: 자연스럽게 처리 가능
  - 빠른 예측: 학습 후 예측이 매우 빠름
- 단일 모델의 한계
  - 과적합 경향: 훈련 데이터의 노이즈까지 완벽히 학습
  - 불안정성(High Variance): 데이터의 작은 변화에도 전혀 다른 트리 생성, 분산이 매우 높다.
  - 축 평행 분할: 대각선 결정 경계를 효과적으로 표현 못함
  - 단일 트리의 한계: 복잡한 패턴을 하나의 트리로 완벽히 포착하기 어려움

→ Decision Tree는 직관적이고 강력한 모델이지만, 단일 모델로 사용하기에는 치명적인 약점들이 있다.

## 2. 앙상블 학습(Ensemble Learning)이란?

> 여러 개의 모델을 결합하여 더 강력한 모델을 만드는 방법

- 개별 모델의 부족한 점을 서로 보완하여 전체적인 예측 성능을 높이고 과적합을 줄이는 것이 목적

### 2-1. 앙상블 학습의 주요 기법

- **Bagging**
  - 같은 알고리즘을 사용하되 데이터 샘플을 다르게 해서 여러 모델을 학습시키는 방식
- **Boosting**
  - 여러 개의 약한 학습기를 순차적으로 학습시키며, 앞에서 틀린 데이터에 가중치를 부여해 다음 모델이 더 잘 맞추도록 하는 방식
- **Voting**
  - 서로 다른 알고리즐 모델들을 모아서 투표를 통해 최종 결과를 결정하는 방식
- **Stacking**
  - 여러 모델이 예측한 결과를 다식 학습 데이터로 사용하여 최종 모델이 한 번 더 학습하는 방식

## 3. Bagging이란?

> **B**ootstrap + **Agg**regat**ing**의 약자로  
> 여러 개의 약한 학습기를 결합하여 하나의 강한 학습기를 만드는 기법

- Bootstrap (복원 추출)
  - 주어진 전체 데이터셋에서 중복을 허용하여 무작위로 $N$개의 샘플을 추출하는 과정
  - 같은 샘플이 여러 번 선택될 수 있음
  - 어떤 샘플은 한 번도 선택되지 않을 수 있음

- Aggregating (집계)
  - 생성된 여러 개의 모델이 내놓은 결과들을 하나로 합치는 과정
  - 분류: 다수결 투표
  - 회귀: 산술 평균

## 4. Out-of-Bag (OOB)란?

> Bootstrap 샘플링 과정에서 중복 허용 추출을 하다 보면 **선택되지 않은 데이터(약 36.8%)** 가 남게된다.  
> 이를 Out-of-Bag(OOB) 데이터라고 부른다.

- 데이터 개수 $n$이 충분히 클 때, 특정 샘플이 한 번의 추출에서 선택되지 않을 확률은 $(1 - \frac{1}{n})^n \approx \frac{1}{e} \approx 0.368$
- Cross-Validation 없이도 성능 평가가 가능하다.
- 학습에 전혀 참여하지 않은 데이터로 평가하므로 신뢰도가 높다.
- OOB Error: OOB 데이터를 이용하여 모델의 성능을 평가한 오차

```
- Bagging의 핵심 목적은 분산을 줄이는 것
- 여러 독립적인 확률 변수의 평균을 구하면, 그 분산은 개별 분산의 1/k로 줄어든다.
- 개별 트리는 과적합되어 불안정하더라도, 이들의 평균을 내면 튀는 결과들이 상쇄되어 전체적인 모델은 안정적인 예측력을 갖게된다.
```

## 5. Random Forest

> Bagging에 Feature Randomness를 추가하여 트리 간의 상관관계를 끊어낸 알고리즘

- Bagging의 한계: 트리들 간의 높은 상관관계
  - 매우 강한 예측력을 가진 특성이 있으면, 모든 트리가 그 특성을 루트 노드 근처에서 선택하게 된다.
  - Bootstrap 샘플이 달라도 트리 구조가 비슷해진다.
  - 트리들이 비슷한 예측을 하면 앙상블 효과가 감소한다.
  - 해결책 : Feature Randomness

### 5-1. Feature Randomness

> 각 노드에서 분할을 결정할 때 모든 특성이 아닌 무작위로 선택된 일부 특성만 고려하는 것

- 기존 Bagging은 노드를 분할할 때 모든 특성(Feature)을 검토한다.
- 반면 Random Forest는 노드 분할 시 전체 특성 중 무작위로 일부 특성만 선택하고, 그중에서 최적의 분할을 찾는다.
- 이 과정을 통해 각 트리는 서로 다른 특성에 주목하게 되고, 결과적으로 매우 독립적인 트리들의 집합이 된다.
  - 같은 노드 위치라도 트리마다 다른 특성 선택
  - 강한 예측 변수가 모든 트리에서 사용되지 않음
  - 약한 예측 변수도 기회를 얻음

### 5-2. Random Forest의 Aggregation

- 수많은 독립적인 트리들이 예측한 결과를 평균(회귀)하거나 다수결(분류)로 통합한다.
- 집단 지성의 원리를 극대화하여 단일 트리의 과적합 문제를 보완한다.

### 5-3. Random Forest의 주요 하이퍼파라미터

- **max_features** : 각 노드에서 고려할 특성 개수
  - 'sqrt' 또는 'auto' : 제곱근, 분류 문제 기본값
  - 0.33 또는 p/3 : 전체의 1/3, 회귀 문제
  - log2 : log₂(p), 특성이 매우 많을 때
  - None 또는 1.0 : 모든 특성, 일반 Bagging
  - 정수 (예: 10) : 정확히 10개, 직접 지정

- **n_estimators** : 결정 트리의 개수
  - 100 : (scikit-learn 기본값)
  - 100 ~ 500 : 실무 권장
  - 많을수록 성능은 좋아지나 계산 시간이 늘어남

- **max_depth**: 트리의 최대 깊이
  - 기본값 : None
  - 과적합 시 : 10 ~ 20으로 제한
  - 과소적합 시 : None 유지
- **min_samples_split**: 노드 분할 최소 샘플 수
  - 기본값 : 2
  - 과적합 시 : 5 ~ 10 증가
  - 과소적합 시 : 2 유지
- **min_samples_leaf**: 리프 노드 최소 샘플 수
  - 기본값 : 1
  - 과적합 시 : 2 ~ 5 증가
  - 과소적합 시 : 1 유지
- **oob_score** : OOB 사용 여부
- **n_jobs** : 병렬 처리에 사용할 CPU 코어 수

```
일반적으로 Random Forest는 개별 트리의 과적합을 걱정하지 않는다.
- 깊은 트리 = 낮은 편향
- 많은 트리 + 무작위성 = 분산 감소
→ 곽적합에 강건한 모델
```

### 5-4. Random Forest의 Feature Importance

> 각 특성이 예측에 얼마나 기여하는지를 나타내는 지표

- 각 노드에서 특정 변수를 사용해 데이터를 분할했을 때, 불순도가 얼마나 줄어들었는지를 계산한다.
- 계산이 매우 빠르고, 모델 학습 과정에서 자동으로 산출된다.

### 5-5. Random Forest의 사용 시 고려사항

```
Random Forest의 강점
- 과적합에 매우 강함
- 하이퍼파라미터 튜닝이 상대적으로 쉬움
- Feature Importance 제공
- 결측치 처리 가능 (일부 구현)
- 이상치에 강건함
- 병렬처리로 빠른 학습
- 스케일링 불필요

Random Forest의 한계
- 메모리 사용량이 많음 (모든 트리 저장)
- 실시간 예측이 느릴 수 있음
- 선형 관계에는 비효율적
- 고차원 희소 데이터에 약함
- 해석이 어려움
- 외삽(Extrapolation) 불가능

사용 시기
- 베이스라인 모델이 필요할 때
- 과적합이 우려될 때
- 특별한 튜닝 없이 좋은 성능을 원할 때
- Feature Importance가 필요할 떄
- 병렬 처리가 가능한 환경일 때
```
