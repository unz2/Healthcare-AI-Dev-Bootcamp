# 머신러닝 심화 (차원 축소 & 군집화 심화)

> 🗓️ **2025-11-10**  
> ✍🏼 **작성자 : unz**

## 📝 목차

1. SVD란?
2. SVD를 이용한 차원 축소 및 이미지 압축
3. Mean Shift
4. DBSCAN
5. 군집화 알고리즘 비교

---

## 1. SVD란?

> Singular Value Decomposition  
> 모든 $m \times n$ 행렬 $A$에 대해 다음과 같이 3개의 행렬의 곱으로 분해하는 선형대수 기법

```
A = U Σ VT

A: 원본 행렬 (m × n)
U: 왼쪽 특이벡터 (m × m)
Σ: 특잇값 대각행렬 (m × n)
VT: 오른쪽 특이벡터 (n × n)
```

**$U$ (Left Singular Vectors)**

- 원본 데이터의 행(row)의 패턴을 나타냄
- $A A^T$를 고유값 분해해서 얻은 고유벡터들로 구성됨

**$\Sigma$ (Singular Values)**

- 해당 축(벡터)이 원본 데이터를 설명하는데 있어서 얼마나 중요한지를 나타냄
- 대각 행렬, 대각 성분에 특이값들이 내림차순으로 정렬
- 특이값을 제외한 나머지는 모두 0

**$V^T$ (Right Singular Vectors)**

- 원본 데이터의 열(column)의 패턴을 나타냄
- $A^T A$를 고유값 분해해서 얻은 고유벡터들의 전치 행렬

## 2. SVD를 이용한 차원 축소 및 이미지 압축

### 2-1. 차원 축소 방법 (Truncated SVD)

> 가장 큰 상위 $k$개의 특이값만을 남기고 나머지는 0으로 취급하여 행렬을 복원하는 방식  
> 이를 통해 데이터의 핵심 구조는 유지하면서 노이즈를 제거하고 용량을 줄일 수 있다.

**원본 확인**  
원본: 1000 × 1000 이미지 = 1,000,000개의 픽셀 값  
→ 총 100만 개의 숫자로 이루어진 행렬은 너무 크고, 전송하기가 어려움  
→ SVD로 압축해서 해결하기

**Full SVD**  
U : 1000 x 1000 = 1,000,000  
Σ : 1000 x 1000 = 1,000,000  
VT : 1000 x 1000 = 1,000,000  
→ 총 3,000,000개, 원본 보다 3배 더 크다  
→ Truncated SVD 사용한다

**SVD 분해 후**  
특잇값: [850, 320, 95, 12, 3, 1, 0.5, ...]  
→ 앞의 3개가 압도적으로 크다

**상위 3개만 사용 (k=3)**  
U : 1000 × 3 = 3,000  
Σ : 3 × 3 = 9  
VT : 3 × 1000 = 3,000  
= 총 6,009개 값만 저장  
→ 99.8% 압축하면서도 대부분 정보 유지!

**행렬곱 확인**  
Uk (1000×3) × Σk (3×3) × VkT (3×1000)  
= (1000×3) × (3×1000)  
= 1000 × 1000  
→ 여전히 원본과 같은 크기로 복원됨!

**어떻게 이렇게 줄일 수 있을까?**  
→ 특잇값 대부분은 매우 작기 때문  
특잇값: [850, 320, 95, 12, 3, 1, 0.5, ...]  
상위 3개 합: 850 + 320 + 95 = 1,265  
나머지 997개 합: 약 20  
→ 상위 3개가 전체의 98%!

**결론**  
상위 3개 특잇값만 사용한다 = 행렬을 구성하는 '중요한 패턴 3개만 남긴다'  
→ U는 1000×3, Σ는 3×3, VT는 3×1000  
→ 차원 축소 & 압축 완성!

### 2-2. SVD로 이미지 파일 크기 줄이기

- 이미지는 픽셀 값을 갖는 행렬로 볼 수 있다.
  - 흑백 이미지 : 픽셀 값의 2차원 배열
  - 컬러 이미지 : 3개 채널(R, G, B)
- 따라서, SVD를 적용해 작은 특이값들을 버리면 이미지를 압축할 수 있다.

```
Step 1: 이미지를 행렬로 로드
Step 2: SVD 분해 → U, Σ, VT
Step 3: 특잇값 상위 k개만 선택
Step 4: 압축된 행렬로 재구성
Step 5: 원본과 비교

장점 : 파일 크기 대폭 감소, 중요 특징 보존, k값으로 압축률 조정 가능
단점 : 완벽한 복원 불가, 손실 압축 방식, JPEG보다 효율 낮음
```

## 3. Mean Shift

> 데이터의 밀도가 가장 높은 곳(Centroid)을 향해 데이터 포인트들을 이동시키며 군집화를 수행하는 알고리즘

### 3-1. 동작 방식

1. 모든 데이터 점을 시작점으로 지점, 각 점마다 독립적으로 이동 시작
2. 각 점 주변에 원형 윈도우(Bandwidth) 설정
3. 윈도우 내 모든 점들의 가중 평균 계산
4. 계산된 평균 지점으로 중심점을 이동
5. 중심점이 더 이상 변하지 않을 때까지 반복
6. 비슷한 최종 위치로 수렴한 점들 = 같은 군집

### 3-2. 핵심 파라미터

- Bandwidth

  - 데이터를 탐색할 '반경'의 크기
  - 값이 너무 크면 → 적은 군집, 과도한 병합
  - 값이 너무 작으면 → 많은 군집, 노이즈 민감

## 4. DBSCAN

> Density-Based Spatial Clustering of Applications with Noise  
> 데이터의 밀도를 기반으로 하는 군집화 알고리즘

- 밀도가 높은 지역 : 군집으로 인식
- 밀도가 낮은 지역 : 노이즈로 인식

### 4-1. 동작 방식

1. 방문하지 않은 임의의 점 P 선택
2. P의 eps 반경 내에 있는 점들의 개수 확인
3. 이웃의 개수가 min_samples 이상이면 P를 핵심 포인트로 지정하고 새로운 군집 만들기
4. 핵심 포인트 P의 이웃들에 대해서 똑같은 과정 반복
5. 이웃 점들 중 핵심 포인트가 있다면 모두 같은 군집으로 흡수
6. 더 이상 군집을 확장할 수 없을 때까지 반복
7. 모든 점을 방문하면 알고리즘 종료

### 4-2. 핵심 파라미터

- eps(epsilon): 이웃으로 간주할 거리
  - 너무 작으면 : 모든 점이 노이즈
  - 너무 크면 : 모든 점이 하나의 군집
- min_samples: 하나의 군집을 형성하기 위한 최소 이웃 개수
  - 작게 : 작은 군집도 탐지 (노이즈 많아짐)
  - 크게 : 큰 군집만 탐지 (노이즈 적어짐)

### 4-3. 점 타입

- Core Point(핵심점) : eps 내에 min_samples 이상의 이웃 → 군집의 중심 역할
- Border Point(경계점) : Core Point의 이웃 안에 있으나 자신은 min_samples 만족하지 못함 → 군집의 가장자리
- Noise Point(이상치) : 어떤 Core Point에도 속하지 않음

## 5. 군집화 알고리즘 비교

| 비교         | K-Means        | Mean Shift          | DBSCAN                      |
| ------------ | -------------- | ------------------- | --------------------------- |
| 군집 수 결정 | 사전 설정 필요 | 자동으로 결정       | 자동으로 결정               |
| 군집 형태    | 원형에 최적    | 밀도가 높은 곳 중심 | 기하하적/불규칙적 형태 가능 |
| 이상치 처리  | 이상치에 취약  | 상대적으로 강함     | 노이즈로 명확히 분리        |

### 5-1. 군집화 선택 가이드

데이터 특성

- 원형 군집 모양, 균일한 데이터 분포 : K-Means
- 복잡한 데이터 모양 : DBSCAN, Mean Shift
- 타원형 모양, 확률 필요 : GMM
- 계층 구조 중요 : Hierarchical

데이터 크기

- 소규모 (< 10K): 모든 방법 가능
- 중규모 (10K-100K): K-means, DBSCAN, GMM
- 대규모 (> 100K): K-means, Mini-Batch K-means

군집 수($K$)

- 알고 있다 : K-means
- 모른다 : DBSCAN, Mean Shift, Hierarchical

노이즈/이상치

- 많다 : DBSCAN
- 없음/처리됨 : K-means

추천 워크플로우

- 1. K-means로 대략적 군집 파악
- 2. DBSCAN 으로 개선
- 3. Hierarchical로 계층 구조 확인
- 4. 도메인 지식과 결과 비교
