# ë¨¸ì‹ ëŸ¬ë‹ ì‹¬í™” (ëª¨ë¸ ìµœì í™”)

> ğŸ—“ï¸ **2025-11-11**  
> âœğŸ¼ **ì‘ì„±ì : unz**

## ğŸ“ ëª©ì°¨

1. ê²½ì‚¬í•˜ê°•ë²•
2. ê²½ì‚¬í•˜ê°•ë²•ì˜ ì¢…ë¥˜
3. Momentum
4. Adam
5. Optimizer ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ
6. í•˜ì´í¼íŒŒë¼ë¯¸í„°
7. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë°©ë²•
8. GridSearchCV
9. RandomSearchCV
10. K-Fold CV

---

## 1. ê²½ì‚¬í•˜ê°•ë²•

> ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ì†ì‹¤ì´ ì¤„ì–´ë“œëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜

1. ë°ì´í„°ë¡œ Loss ê³„ì‚°
2. Gradient ê³„ì‚°
3. Weight ì—…ë°ì´íŠ¸ (W = W - learning_rate Ã— gradient)
4. ë°˜ë³µ

### 1-1. í•µì‹¬ ìš©ì–´

- **Batch** : í•œ ë²ˆì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ë¬¶ìŒ
- **Batch Size** : í•˜ë‚˜ì˜ Batchì— í¬í•¨ëœ ë°ì´í„° ìƒ˜í”Œì˜ ê°œìˆ˜
- **Learning Rate** : ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì´ë™í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” ë³´í­
- **Epoch** : ì „ì²´ í•™ìŠµ ë°ì´í„°ì…‹ì´ ëª¨ë¸ì„ í•œ ë²ˆ í†µê³¼í•œ íšŸìˆ˜

```
ì˜ˆì‹œ)
Total Data : 1,000ê°œ
Batch Size : 100
Learning Rate : 0.01
Epochs : 10

Epoch 1:
    Batch 1 (ìƒ˜í”Œ 1 ~ 100) â†’ Loss ê³„ì‚° â†’ Weight ì—…ë°ì´íŠ¸ (1ë²ˆì§¸)
    Batch 2 (ìƒ˜í”Œ 101 ~ 200) â†’ Loss ê³„ì‚° â†’ Weight ì—…ë°ì´íŠ¸ (2ë²ˆì§¸)
    Batch 3 (ìƒ˜í”Œ 201 ~ 300) â†’ Loss ê³„ì‚° â†’ Weight ì—…ë°ì´íŠ¸ (3ë²ˆì§¸)
    ...
    Batch 10 (ìƒ˜í”Œ 901 ~ 1000) â†’ Loss ê³„ì‚° â†’ Weight ì—…ë°ì´íŠ¸ (10ë²ˆì§¸)

Epoch 2:
    Batch 1 (ìƒ˜í”Œ 1 ~ 100) â†’ Loss ê³„ì‚° â†’ Weight ì—…ë°ì´íŠ¸ (11ë²ˆì§¸)
    Batch 2 (ìƒ˜í”Œ 101 ~ 200) â†’ Loss ê³„ì‚° â†’ Weight ì—…ë°ì´íŠ¸ (12ë²ˆì§¸)
    Batch 3 (ìƒ˜í”Œ 201 ~ 300) â†’ Loss ê³„ì‚° â†’ Weight ì—…ë°ì´íŠ¸ (13ë²ˆì§¸)
    ...
    Batch 10 (ìƒ˜í”Œ 901 ~ 1000) â†’ Loss ê³„ì‚° â†’ Weight ì—…ë°ì´íŠ¸ (20ë²ˆì§¸)

Epoch 10ê¹Œì§€ ë°˜ë³µ

ì´ ì—…ë°ì´íŠ¸ íšŸìˆ˜ = 10 (Epochs) * 10 (Batches) = 100ë²ˆ
```

## 2. ê²½ì‚¬í•˜ê°•ë²•ì˜ ì¢…ë¥˜

```
ë§Œì•½ 100ë§Œ ê°œì˜ í›ˆë ¨ë°ì´í„°ê°€ ìˆë‹¤ë©´?
ë°©ë²• 1 : ì „ì²´ 100ë§Œ ê°œë¥¼ í•œë²ˆì— ì‚¬ìš© â†’ ë©”ëª¨ë¦¬ ë¶€ì¡±, ë„ˆë¬´ ëŠë¦¼
ë°©ë²• 2 : 1ê°œì”© ì‚¬ìš© â†’ ë¶ˆì•ˆì •, GPU í™œìš©í•˜ì§€ ëª»í•¨
ë°©ë²• 3 : ì ë‹¹í•œ í¬ê¸°ë¡œ ë‚˜ëˆ ì„œ ì‚¬ìš© â†’ ì´ê²ƒì´ Batchì˜ ê°œë…

ì „ì²´ ë°ì´í„°ë¥¼ ì‘ì€ ë¬¶ìŒ(Batch)ë¡œ ë‚˜ëˆ„ê³ ,
ê° ë¬¶ìŒ ë§ˆë‹¤ Gradientë¥¼ ê³„ì‚°í•˜ê³  Weight ì—…ë°ì´íŠ¸
```

- **Batch GD (BGD)**
  - ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆì— gradient ê³„ì‚°
  - ì ì  : ì•ˆì •ì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ìˆ˜ë ´ (ì§ì„  ê²½ë¡œ)
  - ë‹¨ì  : ëŒ€ê·œëª¨ ë°ì´í„°ì¼ ê²½ìš° ê³„ì‚° ë¹„ìš©ê³¼ ë©”ëª¨ë¦¬ ë¶€ë‹´ì´ í¬ë‹¤.
- **Stochastic GD (SGD)**
  - ë°ì´í„° 1ê°œë§ˆë‹¤ gradient ê³„ì‚°
  - ì¥ì  : ì†ë„ê°€ ë§¤ìš° ë¹ ë¥´ê³ , Local Minimum íƒˆì¶œì´ ì‰½ë‹¤.
  - ë‹¨ì  : ê²½ë¡œê°€ ë§¤ìš° ë¶ˆì•ˆì •í•˜ë‹¤.
- **Mini-batch GD**
  - ì ì ˆí•œ í¬ê¸°ì˜ ë¬¶ìŒìœ¼ë¡œ gradient ê³„ì‚°
  - ê´€ë¡€ì ì¸ SGD, ì‹¤ë¬´ í‘œì¤€ ë°©ì‹
  - ì¥ì  : ì†ë„ê°€ ë¹ ë¥´ê³ , ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ì¢‹ë‹¤

```
ê²½ì‚¬í•˜ê°•ë²•ì˜ í•œê³„
- Local Minimumì— ë¹ ì§€ë©´ ì•Œê³ ë¦¬ì¦˜ì€ ìµœì ì˜ í•´ë¥¼ ì°¾ì•˜ë‹¤ê³  ì°©ê°í•˜ê³  ë©ˆì¶”ê²Œ ëœë‹¤.
- Gradientê°€ 0ì— ê°€ê¹Œì›Œì§€ë©´ í•™ìŠµì´ ë§¤ìš° ëŠë ¤ì§€ê±°ë‚˜ ë©ˆì¶”ê²Œ ëœë‹¤.
- ì¢ê³  ê¸´ ê³¨ì§œê¸° í˜•íƒœì¼ ê²½ìš° íŒŒë¼ë¯¸í„°ê°€ ìµœì ì ìœ¼ë¡œ ì§ì„  ì´ë™í•˜ì§€ ëª»í•˜ê³  ì§€ê·¸ì¬ê·¸ë¡œ í”ë“¤ë¦¬ë©´ì„œ ìˆ˜ë ´ì´ ëŠë ¤ì§„ë‹¤.

í•´ê²°ì±… Momentum, Adam ë“± ê°œì„ ëœ Optimizer
```

## 3. Momentum

- ê³¼ê±°ì˜ ê¸°ìš¸ê¸°ë¥¼ í˜„ì¬ ì—…ë°ì´íŠ¸ì— ë°˜ì˜í•˜ì—¬ ì§„ë™ì„ ì¤„ì´ê³  ìˆ˜ë ´ì†ë„ë¥¼ ë†’ì¸ë‹¤.
  - ê°™ì€ ë°©í–¥ìœ¼ë¡œ ê³„ì† ê°€ë©´ : ê°€ì†
  - ë°©í–¥ì´ ë°”ë€Œë©´ : ê°ì†
  - Local Minimum íƒˆì¶œ (ê´€ì„±ìœ¼ë¡œ ë„˜ì–´ê°)
  - ì§€ê·¸ì¬ê·¸ ê°ì†Œ

```python
# Scikit-learnì˜ SGDClassifier/SGDRegressorì—ì„œ ì‚¬ìš©
# momentum íŒŒë¼ë¯¸í„°ë¡œ ì¡°ì ˆ ê°€ëŠ¥

from sklearn.linear_model import SGDClassifier

model = SGDClassifier(
    learning_rate='constant',
    eta0=0.01,
    momentum=0.9
)
```

## 4. Adam(Adaptive Moment Estimation)

> Momentumê³¼ RMSPropì˜ ì¥ì ì„ í•©ì¹œ ì•Œê³ ë¦¬ì¦˜

- Momentum (ê´€ì„±)

  - ê°€ë˜ ë°©í–¥ìœ¼ë¡œ ê³„ì† ê°€ë ¤ëŠ” í˜ì„ ë”í•´ì¤€ë‹¤.
  - ë•ë¶„ì— ê²½ì‚¬ê°€ ì™„ë§Œí•œ ê³³ì´ë‚˜ ì•ˆì¥ì ì—ì„œë„ ë©ˆì¶”ì§€ ì•Šê³  ë¹ ë¥´ê²Œ í†µê³¼í•  ìˆ˜ ìˆë‹¤.

- RMSProp (ì ì‘í˜• í•™ìŠµë¥ )
  - ê° íŒŒë¼ë¯¸í„°ë§ˆë‹¤ í•™ìŠµë¥ ì„ ë‹¤ë¥´ê²Œ ì¡°ì ˆí•œë‹¤.
  - ë§ì´ ë³€í™”í•œ íŒŒë¼ë¯¸í„°ëŠ” ë³´í­ì„ ì¤„ì´ê³ , ì ê²Œ ë³€í™”í•œ íŒŒë¼ë¯¸í„°ëŠ” ë³´í­ì„ ë„“í˜€ì„œ ìµœì ì ì— ì„¸ë°€í•˜ê²Œ ì ‘ê·¼í•œë‹¤.

### 4-1. Adam ì‘ë™ ì›ë¦¬

1. 1ì°¨ ëª¨ë©˜íŠ¸ : ê¸°ìš¸ê¸°ì˜ í‰ê·  (Momentum ì—­í• ) â†’ ì§„í–‰ ë°©í–¥ì˜ ê´€ì„± ê²°ì •
2. 2ì°¨ ëª¨ë©˜íŠ¸ : ê¸°ìš¸ê¸° ì œê³±ì˜ í‰ê·  (RMSProp ì—­í• ) â†’ ê° íŒŒë¼ë¯¸í„°ê°€ ì–¼ë§ˆë‚˜ ë³€í•´ì™”ëŠ”ì§€ í¬ê¸° ì¸¡ì •
3. í¸í–¥ ìˆ˜ì •
4. ìµœì¢… ì—…ë°ì´íŠ¸

## 5. Optimizer ë¹„êµ ë° ì„ íƒ ê°€ì´ë“œ

| Optimizer      | íŠ¹ì§•                | ì¥ì                 | ë‹¨ì              | ì¶”ì²œ ìƒí™©                   |
| -------------- | ------------------- | ------------------- | ---------------- | --------------------------- |
| SGD            | ê¸°ë³¸ ê²½ì‚¬í•˜ê°•ë²•     | ê°„ë‹¨, ì´ë¡ ì         | ëŠë¦¼, íŠœë‹ í•„ìš”  | ì‘ì€ ë¬¸ì œ, ì„ í˜• ëª¨ë¸        |
| SGD + Momentum | ê²½ì‚¬í•˜ê°•ë²• ê´€ì„±ì¶”ê°€ | ë¹ ë¥¸ ìˆ˜ë ´, ì•ˆì •ì    | íŠœë‹ í•„ìš”        | ì¼ë°˜ì ì¸ ML ë¬¸ì œ            |
| Adam           | ì ì‘í˜• í•™ìŠµë¥        | ë¹ ë¥¸ ìˆ˜ë ´, ìë™ì¡°ì ˆ | ë©”ëª¨ë¦¬ ì•½ê°„ ë§ìŒ | ë³µì¡í•œ ëª¨ë¸, Neural Network |

## 6. í•˜ì´í¼íŒŒë¼ë¯¸í„°

- **íŒŒë¼ë¯¸í„°** : ëª¨ë¸ì´ í•™ìŠµ ì¤‘ì— ë°°ìš°ëŠ” ê°’ (Weight, Bias ë“±)
- **í•˜ì´í¼íŒŒë¼ë¯¸í„°** : ëª¨ë¸ í•™ìŠµ ì „ ì‚¬ìš©ìê°€ ì§ì ‘ ì„¤ì •í•´ì¤˜ì•¼í•˜ëŠ” ëª¨ë¸ì˜ ì„¤ì • ê°’ (Learning rate, Batch size ë“±)

### 6-1. ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

- Learning Rate (ë²”ìœ„ : 0.0001 ~ 0.1)
- Batch Size (ë²”ìœ„ : 16, 32, 64, 128)
- Regularization (0.001 ~ 10)
- Optimizer (SGD, RMSProp, Adam)
- Epochs

### 6-2. LogisticRegression ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

- C (ì •ê·œí™” ê°•ë„ì˜ ì—­ìˆ˜)
  - ë²”ìœ„: 0.001 ~ 100
  - ì‘ì„ìˆ˜ë¡ ê°•í•œ ì •ê·œí™” â†’ ë‹¨ìˆœí•œ ëª¨ë¸
  - í´ìˆ˜ë¡ ì•½í•œ ì •ê·œí™” â†’ ë³µì¡í•œ ëª¨ë¸
- penalty (ì •ê·œí™” ë°©ì‹)
  - L1 : Lasso (ì¼ë¶€ weightë¥¼ 0ìœ¼ë¡œ)
  - L2 : Ridge (ëª¨ë“  weightë¥¼ ì‘ê²Œ)
- solver (ìµœì í™” ì•Œê³ ë¦¬ì¦˜)
  - liblinear : ì‘ì€ ë°ì´í„°ì— ë¹ ë¦„
  - saga : í° ë°ì´í„°, L1 ì§€ì›
  - lbfgs : ê¸°ë³¸ê°’, ì•ˆì •ì 

```python
# ê¸°ë³¸ ì‚¬ìš© (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ x)
model = LogisticRegression()
model.fit(X_train, y_train)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
model = LogisticRegression(
    c=10,
    penalty='l2',
    solver='liblinear'
)
model.fit(X_train, y_train)

```

## 7. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë°©ë²•

1. **Manual Search (ìˆ˜ë™)**
   - ì§ì ‘ ì—¬ëŸ¬ ê°’ ì‹œë„
   - ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¬ê³ , ì²´ê³„ì ì´ì§€ ì•Šì•„ ê²½í—˜ í•„ìš”í•˜ë‹¤.
2. **Grid Search (ê²©ì íƒìƒ‰)**
   - ëª¨ë“  ì¡°í•© ì‹œë„
   - ë¹ ì§ì—†ì´ íƒìƒ‰í•  ìˆ˜ ìˆìœ¼ë‚˜ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦°ë‹¤.
3. **Random Search (ëœë¤ íƒìƒ‰)**
   - ë¬´ì‘ìœ„ ì¡°í•© ì‹œë„
   - íš¨ìœ¨ì , ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ì— ë” ì§‘ì¤‘í•  ìˆ˜ ìˆë‹¤.

## 8. GridSearchCV

### 8-1. Grid Search ì‘ë™ ë°©ì‹

```python
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# 1. ëª¨ë¸ ì •ì˜
model = LogisticRegression(max_iter=1000, random_state=42)

# 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì •ì˜
param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],    # ì •ê·œí™” ê°•ë„ (ì—­ìˆ˜)
    'penalty': ['l1', 'l2'],                # ì •ê·œí™” ë°©ì‹
    'solver': ['liblinear', 'saga']         # ìµœì í™” ì•Œê³ ë¦¬ì¦˜
}

# 3. GridSearchCV ìƒì„± (5-fold CV)
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=5,                  # êµì°¨ ê²€ì¦ íšŸìˆ˜
    scoring='accuracy',    # í‰ê°€ ì§€í‘œ
    n_jobs=-1,             # ë³‘ë ¬ ì²˜ë¦¬ (ëª¨ë“  CPU ì‚¬ìš©)
    verbose=2              # ì§„í–‰ ìƒí™© ì¶œë ¥
)

# 4. íƒìƒ‰ ì‹¤í–‰
# C: 6ê°œ * penalty: 2ê°œ * solver: 2ê°œ = 24ê°œ ì¡°í•©
# êµì°¨ ê²€ì¦ íšŸìˆ˜ = 5ë²ˆ
# ì´ í•™ìŠµ íšŸìˆ˜ : ì¡°í•© ìˆ˜(24) * êµì°¨ ê²€ì¦ íšŸìˆ˜(5) = 120ë²ˆ
grid_search.fit(X_train, y_train)

# 5. ê²°ê³¼ í™•ì¸
print(f"ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
print(f"ìµœì  ì ìˆ˜: {grid_search.best_score_:.3f}")

# 6. ìµœì  ëª¨ë¸ ì‚¬ìš©
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
```

### 8-2. Grid Search ë¬¸ì œì 

1. íŒŒë¼ë¯¸í„° ê°œìˆ˜ê°€ ëŠ˜ì–´ë‚˜ë©´ ì¡°í•©ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€
2. ì¡°í•©ê³¼ êµì°¨ê²€ì¦ì´ ëŠ˜ì–´ë‚  ìˆ˜ë¡ ì‹œê°„ë„ ì˜¤ë˜ê±¸ë¦¼
3. ì¤‘ìš”í•˜ì§€ ì•Šì€ íŒŒë¼ë¯¸í„°ì—ë„ ê°™ì€ ì‹œê°„ íˆ¬ìí•˜ëŠ” ë¹„íš¨ìœ¨ì„±

## 9. RandomSearchCV

### 9-1. Random Search ì‘ë™ ë°©ì‹

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

# 1. íŒŒë¼ë¯¸í„° ë¶„í¬ ì •ì˜
param_distributions = {
    'n_estimators': randint(50, 200),      # 50~200 ì‚¬ì´ ì •ìˆ˜
    'max_depth': [5, 10, 15, 20, None],    # ë¦¬ìŠ¤íŠ¸ì—ì„œ ì„ íƒ
    'min_samples_split': randint(2, 20),   # 2~20 ì‚¬ì´ ì •ìˆ˜
    'learning_rate': uniform(0.01, 0.2),   # 0.01~0.21 ì‚¬ì´ ì‹¤ìˆ˜
}

# 2. RandomizedSearchCV ìƒì„±
random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_distributions,
    n_iter=50,    # 50ê°œ ì¡°í•©ë§Œ ì‹œë„
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=2
)

# 3. íƒìƒ‰ ì‹¤í–‰
# ì´ í•™ìŠµ íšŸìˆ˜ = 50ë²ˆ
random_search.fit(X_train, y_train)

# 4. ê²°ê³¼
print(f"ìµœì  íŒŒë¼ë¯¸í„°: {random_search.best_params_}")
print(f"ìµœì  ì ìˆ˜: {random_search.best_score_:.3f}")
```

### 9-2. Grid Search ëŒ€ë¹„ ì¥ì 

1. ì‹œê°„ ì¡°ì ˆ ê°€ëŠ¥ (n_iter ì§€ì •)
2. ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ì— ë” ë§ì€ ê°’ ì‹œë„í•  ìˆ˜ ìˆìŒ
3. ì—°ì†ì ì¸ ë²”ìœ„ ì„ íƒ ê°€ëŠ¥
4. ê°™ì€ ì‹œê°„ì— ë” ë„“ì€ ë²”ìœ„ íƒìƒ‰ ê°€ëŠ¥

## 10. K-Fold Cross Validation

> ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ê°œì˜ ì„œë¸Œì…‹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¼ë¶€ëŠ” í•™ìŠµì— ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ê²€ì¦ì— ì‚¬ìš©í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë°©ë²•

- ë‹¨ìˆœíˆ train_test_split ìœ¼ë¡œ ë°ì´í„°ë¥¼ í•œ ë²ˆë§Œ ë‚˜ëˆ„ë©´ í•œê³„ê°€ ë°œìƒí•œë‹¤.
  - ë°ì´í„° í¸í–¥ : ìš°ì—°íˆ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ì‰¬ìš´ ë°ì´í„°ë§Œ ë“¤ì–´ê°”ë‹¤ë©´ ì„±ëŠ¥ì´ ê³¼í•˜ê²Œ ë†’ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤.
  - ë°ì´í„° ë¶€ì¡± : í•™ìŠµ ë°ì´í„°ê°€ ì ì€ ê²½ìš°, íŠ¹ì • ë°ì´í„°ì…‹ì—ë§Œ ê³¼ì í•©ë  ìœ„í—˜ì´ í¬ë‹¤.
- ì „ì²´ ë°ì´í„°ì…‹ì„ ìµœì†Œ í•œ ë²ˆì”©ì€ ê²€ì¦ì— ì‚¬ìš©í•¨ìœ¼ë¡œì¨, ëª¨ë¸ì´ ë°ì´í„°ì˜ ëª¨ë“  íŠ¹ì„±ì„ ì˜ íŒŒì•…í–ˆëŠ”ì§€ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆë‹¤.

### 10-1. K-Fold

- ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ” í˜•íƒœ
- íšŒê·€ ë¬¸ì œë‚˜ ë°ì´í„°ê°€ ê³¨ê³ ë£¨ ì„ì—¬ ìˆëŠ” ê²½ìš°ì— ì£¼ë¡œ ì‚¬ìš©í•œë‹¤.
- ëª¨ë“  ë°ì´í„°ê°€ Train/Testë¡œ ì‚¬ìš©ëœë‹¤.

```
ì‘ë™ ë°©ì‹ (5-Fold)
Fold 1: [Test][Train][Train][Train][Train]
Fold 2: [Train][Test][Train][Train][Train]
Fold 3: [Train][Train][Test][Train][Train]
Fold 4: [Train][Train][Train][Test][Train]
Fold 5: [Train][Train][Train][Train][Test]
```

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')

print(f"ì ìˆ˜: {scores}")
print(f"í‰ê· : {scores.mean():.3f} (Â±{scores.std():.3f})")
```

### 10-2. Stratified K-Fold

- ë¶„ë¥˜ ë¬¸ì œì—ì„œ íŠ¹ì • í´ë˜ìŠ¤ ë¹„ìœ¨ì´ ë·¸ê· í˜•í•  ë•Œ ì‚¬ìš©í•œë‹¤.

```
ì•”ì§„ë‹¨ ë°ì´í„°ì—ì„œ ì•”í™˜ìê°€ 10%, ì •ìƒ 90% ì¸ ê²½ìš°
â†’ ì¼ë°˜ K-FoldëŠ” ì–´ë–¤ í´ë“œì— ì•”í™˜ìê°€ ì „í˜€ í¬í•¨ë˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.

ê° Foldì— ê°™ì€ ë¹„ìœ¨ë¡œ í´ë˜ìŠ¤ ë¶„ë°°
â†’ ëª¨ë“  Foldì—ì„œ ì•”:ì •ìƒ = 1:9 ë¹„ìœ¨ ìœ ì§€
```

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for train_idx, test_idx in skf.split(X, y):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # í´ë˜ìŠ¤ ë¹„ìœ¨ í™•ì¸
    print(f"Train: {np.bincount(y_train)}")
    print(f"Test:  {np.bincount(y_test)}")

    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
```

### 10-3. Time Series Split

- ì£¼ì‹ ê°€ê²©, ë‚ ì”¨, ë¡œê·¸ ë°ì´í„°ì™€ ê°™ì´ ì‹œê°„ ìˆœì„œê°€ ì¤‘ìš”í•œ ë°ì´í„°ì— ì‚¬ìš©í•œë‹¤.
- ë¯¸ë˜ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ê³¼ê±°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°ì´í„° ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ê²€ì¦ì„¸íŠ¸ê°€ í•­ìƒ í•™ìŠµ ì„¸íŠ¸ë³´ë‹¤ ì‹œê°„ì ìœ¼ë¡œ ë’¤ì— ìœ„ì¹˜í•˜ë„ë¡ ì„¤ê³„í•œë‹¤.

```
Fold 1: [Train            ][Test]
Fold 2: [Train                  ][Test]
Fold 3: [Train                        ][Test]
Fold 4: [Train                              ][Test]
```

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

for train_idx, test_idx in tscv.split(X):
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    # ì‹œê°„ ìˆœì„œ í™•ì¸
    print(f"Train: {train_idx[0]}~{train_idx[-1]}")
    print(f"Test:  {test_idx[0]}~{test_idx[-1]}")
```

### 10-4. K ê°’ ì„ íƒí•˜ê¸°

| K ê°’                   | ì¥ì                        | ë‹¨ì                  | ì‚¬ìš©ì‹œê¸°                     |
| ---------------------- | -------------------------- | -------------------- | ---------------------------- |
| 3-5                    | ë¹ ë¦„<br>ì ë‹¹í•œ ì •í™•ë„      | í‰ê°€ ë¶ˆì•ˆì •          | ë¹ ë¥¸ ì‹¤í—˜<br>í° ë°ì´í„°ì…‹     |
| 5-10                   | ê· í˜•<br>ì•ˆì •ì              | -                    | ì¼ë°˜ì  ì¶”ì²œ                  |
| 10+                    | ë§¤ìš° ì•ˆì •ì <br>ì •í™•í•œ í‰ê°€ | ëŠë¦¼                 | ì‘ì€ ë°ì´í„°ì…‹<br>ìµœì¢… í‰ê°€   |
| LOO<br>(Leave-One-Out) | ìµœëŒ€í•œ í™œìš©                | ë§¤ìš° ëŠë¦¼<br>ë¶„ì‚° í¼ | ë§¤ìš° ì‘ì€ ë°ì´í„°<br>(<100ê°œ) |

### 10-5. CV ì£¼ì˜ ì‚¬í•­

- **ë°ì´í„° ëˆ„ìˆ˜** : ìŠ¤ì¼€ì¼ë§ì´ë‚˜ ì „ì²˜ë¦¬ë¥¼ ì „ì²´ ë°ì´í„°ì— ë¨¼ì € ìˆ˜í–‰í•˜ê³  CVë¥¼ ì§„í–‰í•˜ëŠ” ê²½ìš°

```python
# ì˜ëª»ëœ ë°©ë²•
scaler.fit(X_all)
X_scaled = scaler.transform(X_all)
cross_val_score(model, X_scaled, y)  # ëˆ„ì¶œ

# ì˜¬ë°”ë¥¸ ë°©ë²•
for train_idx, test_idx in kfold.split(X):
    scaler.fit(X[train_idx])  # Trainìœ¼ë¡œë§Œ
    X_train = scaler.transform(X[train_idx])
    X_test = scaler.transform(X[test_idx])
```

- **shuffle** : ë°ì´í„°ê°€ ì •ë ¬ë˜ì–´ ìˆëŠ” ê²½ìš° `shuffle=True` ë¥¼ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì´ íŠ¹ì • í´ë˜ìŠ¤ë§Œ í•™ìŠµí•˜ê²Œ ë  ìœ„í—˜ì´ ìˆë‹¤. (TimeSeriesSplitì—ì„œëŠ” `shuffle=True` í•˜ë©´ ì‹œê°„ ìˆœì„œ ìœ ì§€)

### 10-6. ì‹¤ì „ CV ì „ëµ

1. ë¹ ë¥¸ ì‹¤í—˜ ë‹¨ê³„

   - 3-Fold CV
   - ë¹ ë¥´ê²Œ ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ
   - ëŒ€ëµì ì¸ ì„±ëŠ¥ íŒŒì•…

2. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

   - 5-Fold CV (GridSearchCV, RandomSearchCV)
   - ì‹œê°„ê³¼ ì •í™•ë„ ê· í˜•

3. ìµœì¢… ëª¨ë¸ í‰ê°€
   - 10-Fold CV ë˜ëŠ” Stratified 10-Fold
   - ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ ì¸¡ì •
   - ë…¼ë¬¸/ë³´ê³ ì„œì— ì‚¬ìš©
4. ë°°í¬ ì „
   - Nested CV (ì´ì¤‘ CV)
   - Hold-out Test setìœ¼ë¡œ ìµœì¢… ê²€ì¦
   - ì‹¤ì œ ë°ì´í„°ë¡œ A/B í…ŒìŠ¤íŠ¸
