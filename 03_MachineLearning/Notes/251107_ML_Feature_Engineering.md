# ë¨¸ì‹ ëŸ¬ë‹ ì‹¬í™” (Feature Engineering)

> ğŸ—“ï¸ **2025-11-07**  
> âœğŸ¼ **ì‘ì„±ì : unz**

## ğŸ“ ëª©ì°¨

1. Feature Engineering
2. Feature Generation
3. Feature Selection
4. Anomaly Detection

---

## 1. Feature Engineering

> ì›ì‹œ ë°ì´í„°ë¡œë¶€í„° ëª¨ë¸ì˜ ì˜ˆì¸¡ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ Featureì„ ìƒì„±, ì„ íƒ, ë³€í˜•í•˜ëŠ” ê³¼ì •

- **Feature Generation** : ê¸°ì¡´ ë³€ìˆ˜ë“¤ì„ ì¡°í•©í•˜ê±°ë‚˜ ë³€í˜•í•˜ì—¬ ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ë§Œë“œëŠ” ê³¼ì •
- **Feature Selection** : ìˆ˜ë§ì€ ë³€ìˆ˜ ì¤‘ ëª¨ë¸ ì„±ëŠ¥ì— ê¸°ì—¬ë„ê°€ ë†’ì€ í•µì‹¬ ë³€ìˆ˜ë§Œì„ ì¶”ë ¤ë‚´ëŠ” ê³¼ì •
- **Anomaly Detection** : ë°ì´í„°ì˜ ì¼ë°˜ì ì¸ íŒ¨í„´ì—ì„œ ë²—ì–´ë‚œ ê°’(ì´ìƒì¹˜)ì„ ì°¾ì•„ë‚´ì–´ ì²˜ë¦¬í•˜ëŠ” ê³¼ì •

### 1-1. Feature Engineeringì´ í•„ìš”í•œ ì´ìœ 

- **ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ** : ì•Œê³ ë¦¬ì¦˜ ìì²´ì˜ ê°œì„ ë³´ë‹¤ ì˜ ì •ì œëœ ë°ì´í„°ê°€ ì„±ëŠ¥ì— ë” í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.
- **ë…¸ì´ì¦ˆ ì œê±°** : ë¶ˆí•„ìš”í•˜ê±°ë‚˜ ì¤‘ë³µëœ ì •ë³´ë¥¼ ì œê±°í•˜ì—¬ ëª¨ë¸ì´ ë°ì´í„°ì˜ ë³¸ì§ˆì ì¸ íŒ¨í„´ì— ì§‘ì¤‘í•˜ê²Œ í•©ë‹ˆë‹¤.
- **í•´ì„ ê°€ëŠ¥ì„± ì¦ëŒ€** : ë³µì¡í•œ ë°ì´í„°ë¥¼ ì§ê´€ì ì¸ íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë¶„ì„ ê²°ê³¼ë¥¼ ì´í•´í•˜ê¸° ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤.

## 2. Feature Generation

**ì™œ ìƒˆë¡œìš´ Featureë¥¼ ë§Œë“¤ê¹Œ?**

> ì›ë³¸ Featureë§Œìœ¼ë¡œëŠ” ë¹„ì„ í˜• ê´€ê³„ë‚˜ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ í¬ì°©í•˜ê¸° ì–´ë µê¸° ë•Œë¬¸ì—  
> ì‚¬ëŒì´ ëª…ì‹œì ìœ¼ë¡œ ê´€ê³„ë¥¼ ì •ì˜í•´ì£¼ë©´ ëª¨ë¸ì€ í›¨ì”¬ ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

### 2-1. Binning

> ì—°ì†í˜• ë³€ìˆ˜ë¥¼ íŠ¹ì • ê¸°ì¤€ì— ë”°ë¼ ë²”ì£¼í˜• ë³€ìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ë²•

- Equal Width Binning: ê°™ì€ ë„ˆë¹„ë¡œ êµ¬ê°„ ë‚˜ëˆ„ê¸° (ì˜ˆ: [0-10], [10-20], [20-30])
- Equal Frequency Binning: ê° êµ¬ê°„ì— ë™ì¼í•œ ë°ì´í„° ê°œìˆ˜ê°€ ë“¤ì–´ê°€ë„ë¡ êµ¬ê°„ ë‚˜ëˆ„ê¸°
- Custom Binning: ë„ë©”ì¸ ì§€ì‹ì— ë”°ë¼ ì„ì˜ë¡œ ê²½ê³„ê°’ì„ ì„¤ì •

ì–¸ì œ ì‚¬ìš© í• ê¹Œ?

- ë¹„ì„ í˜• ê´€ê³„ë¥¼ ë‹¨ìˆœí™”í•  ë•Œ
- ì´ìƒì¹˜ì˜ ì˜í–¥ì„ ì¤„ì¼ ë•Œ
- í•´ì„ ì‚¬ëŠ¥ì„±ì„ ë†’ì¼ ë•Œ

```python
# ë°ì´í„° ìƒì„±
ages = np.array([22, 25, 35, 45, 52, 58, 67, 72]).reshape(-1, 1)

# Equal Width Binning
kbd_width = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
age_binned_width = kbd_width.fit_transform(ages)
print("Equal Width:", age_binned_width.ravel())

# Equal Frequency Binning
kbd_freq = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='quantile')
age_binned_freq = kbd_freq.fit_transform(ages)
print("Equal Frequency:", age_binned_freq.ravel())

# Custom Binning
df = pd.DataFrame({'age': ages.ravel()})
df['age_group'] = pd.cut(df['age'], bins=[0, 30, 60, 100], labels=['ì²­ë…„', 'ì¤‘ë…„', 'ë…¸ë…„'])
print("Custom Binning:", df['age_group'].values)

# Equal Width: [0. 0. 0. 1. 1. 2. 2. 2.]
# Equal Frequency: [0. 0. 0. 1. 1. 2. 2. 2.]
# Custom Binning: ['ì²­ë…„', 'ì²­ë…„', 'ì¤‘ë…„', 'ì¤‘ë…„', 'ì¤‘ë…„', 'ì¤‘ë…„', 'ë…¸ë…„', 'ë…¸ë…„']
```

### 2-2. Polynomial Features

> ì…ë ¥ ê°’ì˜ ì œê³±ì´ë‚˜ ì„œë¡œ ë‹¤ë¥¸ ë³€ìˆ˜ ê°„ì˜ ê³±ì„ ìƒˆë¡œìš´ Featureë¡œ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì— ë¹„ì„ í˜•ì„±ì„ ë¶€ì—¬í•˜ëŠ” ê¸°ë²•

- ë¹„ì„ í˜• ê´€ê³„ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ì§€ë§Œ, feature ìˆ˜ê°€ ëŠ˜ì–´ë‚˜ë©´ì„œ ê³„ì‚° ë¹„ìš© ì¦ê°€ì™€ Overfitting ìœ„í—˜ì´ ìˆë‹¤.

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np

# ë¹„ì„ í˜• ë°ì´í„° ìƒì„±
X = np.array([[1], [2], [3], [4], [5]])
y = np.array([2, 7, 16, 29, 46])  # y = 2xÂ² - 3x + 2

# 1ì°¨ (ì„ í˜•) ëª¨ë¸
model_linear = LinearRegression()
model_linear.fit(X, y)
print("ì„ í˜• ëª¨ë¸ RÂ²:", model_linear.score(X, y))

# 2ì°¨ ë‹¤í•­ì‹ í”¼ì²˜ ìƒì„±
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
print("ë³€í™˜ëœ í”¼ì²˜:", X_poly[0])  # [1, x, xÂ²]

# 2ì°¨ ë‹¤í•­ì‹ ëª¨ë¸
model_poly = LinearRegression()
model_poly.fit(X_poly, y)
print("ë‹¤í•­ì‹ ëª¨ë¸ RÂ²:", model_poly.score(X_poly, y))

# ì„ í˜• ëª¨ë¸ RÂ²: 0.9557661927330173
# ë³€í™˜ëœ í”¼ì²˜: [1. 1. 1.]
# ë‹¤í•­ì‹ ëª¨ë¸ RÂ²: 1.0
```

|    ë°©ë²•    |      ì‚¬ìš© ì‹œê¸°       |            ì¥ì              |              ë‹¨ì               |
| :--------: | :------------------: | :-------------------------: | :----------------------------: |
|  Binning   | ì—°ì†í˜•ì„ ë²”ì£¼ë¡œ ë³€í™˜ |  í•´ì„ ì‰¬ì›€<br> ì´ìƒì¹˜ ê°•ê±´  | ì •ë³´ ì†ì‹¤<br> êµ¬ê°„ ì„ íƒ ì–´ë ¤ì›€ |
| Polynomial |   ë¹„ì„ í˜• ê´€ê³„ í•™ìŠµ   | í‘œí˜„ë ¥ ì¦ê°€<br> ê°„ë‹¨í•œ êµ¬í˜„ |   í”¼ì²˜ í­ë°œ<br> Overfitting    |

## 3. Feature Selection

**ì™œ Featureë¥¼ ì„ íƒí•´ì•¼ í• ê¹Œ?**

- ì°¨ì›ì˜ ì €ì£¼(Curse of Dimensionality)
  - Feature ìˆ˜ê°€ ì¦ê°€í• ìˆ˜ë¡ ë°ì´í„° ê³µê°„ì´ í¬ì†Œí•´ì§€ê³  ëª¨ë¸ í•™ìŠµì´ ì–´ë ¤ì›Œì§„ë‹¤.
  - Featureê°€ 1000ê°œì¸ë° ì§„ì§œ ì¤‘ìš”í•œ ê±´ 10ê°œ ë¿ â†’ ë¶ˆí•„ìš”í•œ Featureê°€ ë…¸ì´ì¦ˆë¥¼ ë§Œë“¦
  - í•™ìŠµ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ê³ , Overfitting ë°œìƒ
- ì˜¤ì»´ì˜ ë©´ë„ë‚ (Occamâ€™s Razor) : ë¶ˆí•„ìš”í•˜ê²Œ ë³µì¡í•œ ëª¨ë¸ë³´ë‹¤ ë‹¨ìˆœí•œ ëª¨ë¸ì´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ë‹¤ëŠ” ì›ì¹™

### 3-1. Filter Method

> í†µê³„ì  ì²™ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ ê°„ì˜ ì—°ê´€ì„±ì„ í‰ê°€

| ê¸°ë²•                | ì£¼ìš” ìš©ë„                         | íŠ¹ì§•                               |
| ------------------- | --------------------------------- | ---------------------------------- |
| Correlation         | ë³€ìˆ˜ ê°„ ì¤‘ë³µì„± í™•ì¸               | ì„ í˜•ì  ê´€ê³„ ì¸¡ì •                   |
| Chi-square          | ë²”ì£¼í˜• ë°ì´í„°                     | ë°ì´í„°ê°€ ì–‘ìˆ˜ì—¬ì•¼ í•¨               |
| F-statistic (ANOVA) | ì—°ì†í˜• ë…ë¦½ë³€ìˆ˜ + ë²”ì£¼í˜• ì¢…ì†ë³€ìˆ˜ | ê·¸ë£¹ ê°„ í‰ê·  ì°¨ì´ ë¶„ì„             |
| Mutual Information  | ë‹¤ëª©ì                             | ë¹„ì„ í˜• ê´€ê³„ë„ ê°ì§€, ê³„ì‚° ë¹„ìš© ë†’ìŒ |
| Variance Threshold  | ë¹„ì§€ë„ í•™ìŠµí˜• ì„ íƒ                | ê°’ì´ ê±°ì˜ ì¼ì •í•œ ë³€ìˆ˜ ì œê±°         |

```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.feature_selection import chi2, SelectKBest, f_classif, mutual_info_classif, VarianceThreshold

# ë°ì´í„° ë¡œë“œ
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = iris.target

# 1) Correlation
corr_matrix = X.corr()
df = X.copy()
df['target'] = y
target_corr = df.corr()['target'].sort_values(ascending=False)
print("íƒ€ê²Ÿê³¼ì˜ ìƒê´€ê³„ìˆ˜:\n", target_corr)

# 2) Chi-square
# ìƒìœ„ 2ê°œ ë³€ìˆ˜ ì„ íƒ
chi2_selector = SelectKBest(chi2, k=2)
X_kbest = chi2_selector.fit_transform(X, y)
print("Chi-2 ì„ íƒ ë³€ìˆ˜:", X.columns[chi2_selector.get_support()].tolist())

# 3) F-statistic (ANOVA)
f_selector = SelectKBest(f_classif, k=2)
X_f = f_selector.fit_transform(X, y)
print("F-statistic ì„ íƒ ë³€ìˆ˜:", X.columns[f_selector.get_support()].tolist())

# 4) Mutual Information
mi = mutual_info_classif(X, y)
mi_series = pd.Series(mi, index=X.columns)
print("Mutual Information ìŠ¤ì½”ì–´:\n", mi_series.sort_values(ascending=False))

# 5) Variance Threshold
# ë¶„ì‚°ì´ 0.2 ì´í•˜ì¸ ë³€ìˆ˜ ì œê±°
selector = VarianceThreshold(threshold=0.2)
X_var = selector.fit_transform(X)
print("ì„ íƒëœ ë³€ìˆ˜:", X.columns[selector.get_support()].tolist())
```

### 3-2. Wrapper Method

> ëª¨ë¸ ì„±ëŠ¥ì„ ê¸°ì¤€ìœ¼ë¡œ Feature ì¡°í•©ì„ í‰ê°€

- **Forward Selection** : Featureë¥¼ í•˜ë‚˜ì”© ì¶”ê°€í•˜ë©° ì„±ëŠ¥ í™•ì¸
- **Backward Elimination** : ëª¨ë“  Featureì—ì„œ í•˜ë‚˜ì”© ì œê±°í•˜ë©° ì„±ëŠ¥ í™•ì¸
- **RFE (Recursive Feature Elimination)** : ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ í›„ ì¤‘ìš”ë„ê°€ ë‚®ì€ ë³€ìˆ˜ë¶€í„° ìˆœì°¨ì ìœ¼ë¡œ ì œê±°

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_breast_cancer

# ë°ì´í„° ë¡œë“œ
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# RFEë¡œ í”¼ì²˜ ì„ íƒ
model = LogisticRegression(max_iter=10000)
rfe = RFE(estimator=model, n_features_to_select=10)
X_rfe = rfe.fit_transform(X, y)

print("ì „ì²´ í”¼ì²˜ ìˆ˜:", X.shape[1])
print("ì„ íƒëœ í”¼ì²˜ ìˆ˜:", X_rfe.shape[1])
print("í”¼ì²˜ ìˆœìœ„:", rfe.ranking_)
print("ì„ íƒëœ í”¼ì²˜:", [cancer.feature_names[i] for i in range(len(rfe.support_)) if rfe.support_[i]])

# ì„±ëŠ¥ ë¹„êµ
from sklearn.model_selection import cross_val_score
score_all = cross_val_score(model, X, y, cv=5).mean()
score_rfe = cross_val_score(model, X_rfe, y, cv=5).mean()
print(f"ì „ì²´ í”¼ì²˜ ì •í™•ë„: {score_all:.3f}")
print(f"ì„ íƒëœ í”¼ì²˜ ì •í™•ë„: {score_rfe:.3f}")
```

### 3-3. Embedded Method

> ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ Feature ì„ íƒì´ ë™ì‹œì— ìˆ˜í–‰ë¨

- **Lasso (L1 Regularization)** : ë¶ˆí•„ìš”í•œ Featureì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ë³€ìˆ˜ë¥¼ ìë™ ì„ íƒ
- **Ridge (L2 Regularization)** : ëª¨ë“  Featureì˜ ê°€ì¤‘ì¹˜ë¥¼ ì‘ê²Œ ë§Œë“¤ì–´ ê³¼ì í•© ë°©ì§€
- **Tree-based Importance** : ê²°ì • íŠ¸ë¦¬ ê³„ì—´ ëª¨ë¸ì´ ë…¸ë“œ ë¶„í•  ì‹œ ì‚¬ìš©í•˜ëŠ” ì§€í‘œ(Gini ë“±)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¤‘ìš”ë„ ì‚°ì¶œ
- **ElasticNet** : Lassoì™€ Ridge ì¥ì  ê²°í•©

```python
from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeClassifier
import numpy as np

# ë°ì´í„° ë¡œë“œ
X, y = load_breast_cancer(return_X_y=True)

# Lasso
lasso = Lasso(alpha=0.01)
lasso.fit(X, y)
lasso_coef = np.abs(lasso.coef_)
print("Lasso - 0ì¸ ê³„ìˆ˜:", np.sum(lasso_coef == 0))

# Tree-based Importanc
rf = DecisionTreeClassifier(max_depth=5, random_state=42)
rf.fit(X, y)
importances = rf.feature_importances_

# ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì²˜
indices = np.argsort(importances)[::-1][:10]
print("Top 10 ì¤‘ìš” í”¼ì²˜:")
for i in indices:
    print(f"{cancer.feature_names[i]}: {importances[i]:.4f}")
```

## 4. Anomaly Detection

**ì™œ ì´ìƒì¹˜ íƒì§€ê°€ í•„ìš”í• ê¹Œ?**

- ì´ìƒì¹˜ëŠ” í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì™œê³¡ì‹œì¼œ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì €í•´í•œë‹¤.
- ëª¨ë¸ì´ ì´ìƒì¹˜ì— ë§ì¶°ì§€ë©´ ì˜ˆì¸¡ ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì§€ê³ , ì˜ëª»ëœ ì˜ì‚¬ê²°ì •ìœ¼ë¡œ ì´ì–´ì§„ë‹¤.
- ì—°ë´‰ ì˜ˆì¸¡ ëª¨ë¸ì—ì„œ í•œ ëª…ì˜ ì˜ëª»ëœ ë°ì´í„°(9999ì–µì›) â†’ ì „ì²´ ì˜ˆì¸¡ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ì•„ì§

### 4-1. ì´ìƒì¹˜(Anomaly)ë€?

- Point Anomaly: ê°œë³„ ë°ì´í„° í¬ì¸íŠ¸ í•˜ë‚˜ê°€ ì „ì²´ ë¶„í¬ì—ì„œ í¬ê²Œ ë²—ì–´ë‚œ ê²½ìš° (ì˜ˆ: í‚¤ 250cm)
- Contextual Anomaly: íŠ¹ì • ìƒí™©(ë§¥ë½)ì—ì„œë§Œ ì´ìƒì¹˜ì¸ ê²½ìš°. (ì˜ˆ: ì—¬ë¦„ ê¸°ì˜¨ ì˜í•˜ 20ë„)
- Collective Anomaly: ê°œë³„ë¡œëŠ” ì •ìƒì¼ ìˆ˜ ìˆìœ¼ë‚˜ ê·¸ë£¹ìœ¼ë¡œ ëª¨ì˜€ì„ ë•Œ ë¹„ì •ìƒì ì¸ íŒ¨í„´ì„ ë³´ì´ëŠ” ê²½ìš° (ì˜ˆ: íŠ¹ì • ì‹œê°„ëŒ€ íŠ¸ë˜í”½ ê¸‰ì¦)
- Types
  - Global : ì „ì²´ ë°ì´í„° ê¸°ì¤€
  - Local : ì£¼ë³€ ë°ì´í„° ê¸°ì¤€

> ì´ìƒì¹˜ vs ì—ëŸ¬  
> ì´ìƒì¹˜ : ì§„ì§œ ë°ì´í„°ì§€ë§Œ í¬ê·€í•¨ (ë°œê²¬ ê°€ì¹˜ ìˆìŒ)  
> ì—ëŸ¬ : ì¸¡ì • ì˜¤ë¥˜ (ì œê±°í•´ì•¼ í•¨)

### 4-2. ì£¼ìš” íƒì§€ ê¸°ë²•

### Z-Score

- í‘œì¤€í¸ì°¨ ê¸°ë°˜ íƒì§€
- ì¼ë°˜ì ìœ¼ë¡œ í‰ê· ìœ¼ë¡œë¶€í„° í‘œì¤€í¸ì°¨ì˜ $|Z| > 3$ ê°’ì„ ì´ìƒì¹˜ë¡œ íŒë‹¨
- ì¥ì  : ê°„ë‹¨í•˜ê³  ì§ê´€ì , ë¹ ë¥¸ ê³„ì‚°, í•´ì„ ì‰¬ì›€
- ë‹¨ì  : ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •, í‰ê· /ë¶„ì‚°ì— ë¯¼ê°, ë‹¤ë³€ëŸ‰ íƒì§€ ì–´ë ¤ì›€
- ì‚¬ìš© ì‹œê¸° : ë°ì´í„°ê°€ ëŒ€ëµ ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥¼ ë•Œ, ë¹ ë¥¸ 1ì°¨ í•„í„°ë§ì´ í•„ìš”í•  ë•Œ

```
Z = (x - mean) / std
Outlier : |Z| > 3
```

### IQR (Interquartile Range)

- ì‚¬ë¶„ìœ„ìˆ˜ ê¸°ë°˜ íƒì§€
- ì¥ì  : ë¶„í¬ ê°€ì • ë¶ˆí•„ìš”, ì¤‘ì•™ê°’ ê¸°ë°˜, Box plot ì§ê´€ì 
- ë‹¨ì  : ë‹¤ë³€ëŸ‰ íƒì§€ ì–´ë ¤ì›€, ê·¹ë‹¨ê°’ì— ë‘”ê°í•  ìˆ˜ ìˆìŒ
- ì‚¬ìš© ì‹œê¸° : ë°ì´í„° ë¶„í¬ë¥¼ ëª¨ë¥¼ ë•Œ, ì¤‘ì•™ê°’ ê¸°ë°˜ íƒì§€ë¥¼ ì›í•  ë•Œ, Box plotìœ¼ë¡œ ì‹œê°í™”í•  ë–„

```
Q1 (25 percentile): í•˜ìœ„ 25%
Q3 (75 percentile): í•˜ìœ„ 75%
IQR = Q3 - Q1

Lower = Q1 - 1.5 Ã— IQR
Upper = Q3 + 1.5 Ã— IQR
Outlier: x < Lower or x > Upper
```

### DBSCAN

- ë°€ë„ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ì–´ëŠ êµ°ì§‘ì—ë„ ì†í•˜ì§€ ì•ŠëŠ” ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ë¥¼ íƒì§€
- ì¥ì  : ë‹¤ë³€ëŸ‰ íƒì§€ ê°€ëŠ¥, êµ°ì§‘ ëª¨ì–‘ ììœ , êµ°ì§‘ ê°œìˆ˜ ì§€ì • ë¶ˆí•„ìš”
- ë‹¨ì  : íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”, ê³ ì°¨ì›ì—ì„œ ì„±ëŠ¥ ì €í•˜, ë°€ë„ ë³€í™” ëŒ€ì‘ ì–´ë ¤ì›€

```
ë°€ë„ê°€ ë†’ì€ ì§€ì—­ = ì •ìƒ ë°ì´í„°
ë°€ë„ê°€ ë‚®ì€ ì§€ì—­ = ì´ìƒì¹˜
êµ°ì§‘ì— ì†í•˜ì§€ ëª»í•œ ì  = Outlier
```

### LOF (Local Outlier Factor)

- ì£¼ë³€ ë°ì´í„°ì™€ì˜ ë°€ë„ë¥¼ ë¹„êµí•˜ì—¬ ìƒëŒ€ì ìœ¼ë¡œ ê³ ë¦½ëœ ì •ë„ë¥¼ ìˆ˜ì¹˜í™”
- ì¥ì  : Local ì´ìƒì¹˜ íƒì§€, ë‹¤ì–‘í•œ ë°€ë„ ëŒ€ì‘, DBSCANë³´ë‹¤ ìœ ì—°
- ë‹¨ì  : ê³„ì‚° ë¹„ìš© ë†’ìŒ, K ì„ íƒ í•„ìš”, ê³ ì°¨ì› ì„±ëŠ¥ ì €í•˜

```
LOF(x) = (ì£¼ë³€ ì´ì›ƒ í‰ê·  ë°€ë„) / (xì˜ ë°€ë„)
LOF > 1: ì£¼ë³€ë³´ë‹¤ ë°€ë„ ë‚®ìŒ (ì´ìƒì¹˜ ê°€ëŠ¥)
LOF â‰ˆ 1: ì£¼ë³€ê³¼ ë¹„ìŠ·í•œ ë°€ë„ (ì •ìƒ)
```
