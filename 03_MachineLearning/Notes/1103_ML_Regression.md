# 머신러닝 기초 (지도학습 - 회귀)

> 🗓️ **2025-11-03**

## ✍🏼 **작성자 : unz**

## 📝 목차

1. 함수(Function)란?
2. 회귀(Regression)란?
3. 선형 회귀(Linear Regression)
4. 다중 선형 회귀(Multiple Linear Regression)
5. 다항 회귀(Polynomial Regression)
6. 오차(Error)와 손실함수(Loss Function)
7. 경사하강법(Gradient Descent)
8. 머신러닝에서 알고리즘(Algorithm)과 모델(Model)의 차이

---

## 1. 함수(Function)란?

- 수학에서 함수(Function)란 입력($x$)과 출력($y$) 사이의 대응 관계를 나타내는 규칙
- 보통 $y = f(x)$라고 표현하며, 특정 값을 넣었을 때 정해진 규칙에 따라 결과값이 나오는 '상자'와 같다.

### 1-1. 머신러닝과 함수

- 머신러닝의 본질은 데이터를 가장 잘 설명하는 함수를 찾는 과정
- 전통적 프로그래밍 : 사람이 직접 규칙(함수)을 작성하여 입력 데이터를 처리한다.
- 머신러닝 : 입력 데이터($x$)와 정답($y$)을 컴퓨터에 주면, 컴퓨터가 그 사이의 관계를 나타내는 최적의 함수 $f$를 찾아낸다.

## 2. 회귀(Regression)란?

- 데이터를 통해 연속적인 숫자 값을 예측하는 것
- 여러 개의 데이터 포인트 사이를 가장 잘 설명하는 하나의 선(또는 함수)를 찾는 과정
  - 독립 변수($x$) : 영향을 주는 변수
  - 종속 변수($y$) : 결과가 되는 변수

## 3. 선형 회귀(Linear Regression)

- 독립 변수($x$)와 종속 변수($y$) 사이의 관계를 직선 형태로 모델링하는 기법
- **목표** : 실제 데이터와 모델이 예측한 직선 사이의 거리(오차) 합을 최소화하는 직선 찾기
- **수식** : $y = wx + b$

### 3-1. 가중치(Weight, $w$)와 편향(Bias, $b$)

- 가중치($w$)
  - 직선의 기울기
  - 입력값이 결과에 미치는 영향력
  - $w$가 클수록 $x$의 변화가 $y$에 큰 변화를 준다.
- 편향($b$)
  - 직선의 y절편
  - 입력값이 0일 때의 기본 출력값
  - 모델이 얼마나 유연하게 데이터에 적합될 수 있는지를 조절한다.

## 4. 다중 선형 회귀(Multiple Linear Regression)

- 실제 세상의 현상은 단 하나의 요인($x$)으로만 결정되지 않는다.
- 집값을 예측할 때 '면적'뿐만 아니라 '방 개수', '역과의 거리' 등 여러 변수가 필요하듯
- 여러 개의 독립 변수를 사용하는 것이 다중 선형 회귀이다.
- **수식** : $y = w_1x_1 + w_2x_2 + \dots + w_nx_n + b$

### 4-1. 다중 선형 회귀의 한계

- 직선으로 표현할 수 없는 관계
  - 속도와 제동거리: 속도의 제곱에 비례
  - 온도와 반응속도: 곡선 형태
  - 경험과 생산성: 처음엔 급증, 나중엔 완만

## 5. 다항 회귀(Polynomial Regression)

- 데이터의 분포가 직선이 아닌 곡선 형태를 띌 때,
- 독립 변수의 차수를 높여(예: $x^2, x^3$) 비선형 관계를 모델링하는 기법

### 5-1. Pipeline이란?

- 여러 단계를 하나로 묶어주는 도구
- 데이터 전처리와 학습 과정을 자동화하여 코드의 가독성과 유지보수성을 높여준다.
- 다항 회귀를 구현할 때는 차수 확장(Feature Polynomial)과 모델 학습(Linear Regression)을 하나의 과정으로 묶어주는 도구

```Python
# Pipline 없이
# 1단계: 변환
poly = PolynomialFeatures(2)
X_poly = poly.fit_transform(X)

# 2단계: 학습
model = LinearRegression()
model.fit(X_poly, y)

# 3단계: 예측
X_new_poly = poly.transform(X_new)
y_pred = model.predict(X_new_poly)
```

```Python
# Pipline 사용
model = make_pipeline(
    PolynomialFeatures(2),
    LinearRegression()
)

model.fit(X, y)
y_pred = model.predict(X_new)
```

## 6. 오차(Error)와 손실함수(Loss Function)

- **오차** : 실제값 $y$와 예측값 $\hat{y}$의 차이 ($y - \hat{y}$)
- **손실함수** : 모델의 예측이 얼마나 틀렸는지를 하나의 숫자로 수치화한 함수

### 6-1. 평균 제곱 오차 (MSE, Mean Squared Error)

- 오차를 제곱하여 평균을 낸 값
- **수식** : $MSE = \frac{1}{n} \sum (y_i - \hat{y}_i)^2$
- 오차에 제곱을 하는 이유 : 음수 오차를 방지하고, 큰 오차에 더 큰 패널티를 주기 위함

### 6-2. 최소제곱법 (Least Squares)

- 실제 데이터 값과 모델이 예측한 값의 차이(잔차)를 제곱하여 그 합이 최소가 되도록 만드는 방법
- MSE를 최소로 만드는 가중치 $w$와 편향 $b$를 찾는 방법
- 주로 선형 회귀(Linear Regression)에서 사용된다.

## 7. 경사하강법(Gradient Descent)

- 손실함수 그래프에서 현재 위치의 기울기(Gradient)를 구하고,
- 기울기가 낮은 방향으로 조금씩 $w$와 $b$를 업데이트하며 최저점(Loss가 최소인 지점)을 찾아가는 최적화 알고리즘

### 7-1. 경사하강법의 원리

1. 시작 : 가중치 $w$를 무작위로 설정
2. 경사 확인 : 미분을 통해 기울기 계산
3. 이동 : 기울기가 낮은 방향으로 이동 (이때 한 걸음의 크기가 학습률(Learning Rate))
4. 반복 : 기울기가 0에 가까워질 떄까지 이 과정을 반복

### 7-2. 학습률(Learning Rate)의 중요성

- **학습률이 너무 작으면** : 매우 천천히 이동, 학습 시간이 오래 걸림
- **학습률이 너무 크면** : 지나치게 많이 이동, 최저점을 지나쳐버림

## 8. 머신러닝에서 알고리즘(Algorithm)과 모델(Model)의 차이

#### 1. 알고리즘(Algorithm): 학습하는 방법

- 데이터를 처리하여 패턴을 찾아내는 절차나 공식 그 자체
- 아직 아무런 데이터도 배우지 않은 논리적 설계도 상태를 의미
- 정해진 규칙에 따라 입력을 출력으로 바꾸는 수학적 방법
- 예시) 선형 회귀, 결정 트리, 랜덤 포레스트, 신경망

#### 2. 모델(Model): 학습의 결과물

- 특정 알고리즘에 실제 데이터를 넣어서 학습을 완료한 상태
- 학습을 통해 결정된 구체적인 가중치와 편향을 가지고 있는 소프트웨어 파일
- 새로운 데이터를 넣었을 떄 예측값을 내놓을 수 있는 완성된 엔진
- 예시) 집값을 예측하도록 학습된 선형 회귀 모델, 개와 고양이를 구별할 수 있게 학습된 VGG-16 모델
